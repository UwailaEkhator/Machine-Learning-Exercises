{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c464d862-be96-4cb3-829e-56eb7e55ae15",
   "metadata": {},
   "source": [
    "# Ekhator Uwailas Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351c048-a2ca-47ef-94b3-dd438b783422",
   "metadata": {},
   "source": [
    "# Question 1 (15 pts)\n",
    "\n",
    "Implement the fit and predict procedures for the logistic regression (scikit is not allowed) with norm 2 regularization function (and Lambda parameter).\n",
    "<br>\n",
    "Use as the input parameters of the gradient ascent the maximum number of iterations (just a constant e.g 100) and the learning factor (e.g. 0.01).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d8c7855-b6ec-4b57-9219-9cf231f8cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf09933-90c4-4d7f-a014-48f9b8234b4b",
   "metadata": {},
   "source": [
    "The sigmoid function is gotten from:\n",
    "$ P(y=1|x,\\theta)=\\frac{1}{1+e^{-z}}$<br>\n",
    "where z = $\\theta^Tx$\n",
    "<br>\n",
    "The log likelihood function is gotten from:\n",
    "$ ln L(\\theta)=\\sum_{i=1}^m y^{(i)}ln l(\\theta^Tx^{(i)}) + \\sum_{i=1}^m 1-y^{(i)}ln l(-\\theta^Tx^{(i)})$<br>\n",
    "where y_true: $y^{(i)}$<br>\n",
    "y_pred: $l(\\theta^Tx^{(i)})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab7a6f9-a95d-435f-bfdd-ac7e4bc777a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionWithL2:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        '''Initialize variables\n",
    "        Args:\n",
    "            learning_rate  : Learning Rate\n",
    "            max_iterations : Max iterations for training weights\n",
    "        '''\n",
    "        # Initialising all the parameters\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.likelihoods    = []\n",
    "        \n",
    "        # Define epsilon because log(0) is undefined\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    #Sigmoid function maps predictions to the range of 0 and 1\n",
    "    def sigmoid(self, z):\n",
    "        '''Sigmoid function: f:R->(0,1)\n",
    "        Args:\n",
    "            z : A numpy array (num_samples,)\n",
    "        Returns:\n",
    "            A numpy array where sigmoid function applied to every element\n",
    "        '''\n",
    "        #\n",
    "        sig_z = (1/(1+np.exp(-z)))\n",
    "        #\n",
    "\n",
    "        #using an assert to statement to check that after the sigmoid function was applied, it did not alter the input data shape\n",
    "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation, data shape altered. Check!'\n",
    "        return sig_z\n",
    "\n",
    "    \n",
    "    def log_likelihood(self, y_true, y_pred):\n",
    "        '''Calculates maximum likelihood estimate\n",
    "        Args:\n",
    "            y_true : Numpy array of actual truth values (num_samples,)\n",
    "            y_pred : Numpy array of predicted values (num_samples,)\n",
    "        Returns:\n",
    "            Log-likelihood, scalar value\n",
    "        '''\n",
    "        # Removing 0 or 1 values in y_pred so that log is not undefined\n",
    "        #this helps to ensure y_pred is within the rang [self.eps, 1-self.eps] to prevent it from being too close to 0 or 1\n",
    "        y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
    "        \n",
    "        #\n",
    "        likelihood = (y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
    "        #\n",
    "        \n",
    "        return np.mean(likelihood)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Trains logistic regression model using gradient ascent\n",
    "        to gain maximum likelihood on the training data\n",
    "        Args:\n",
    "            X : Numpy array (num_examples, num_features)\n",
    "            y : Numpy array (num_examples, )\n",
    "        Returns: VOID\n",
    "        '''\n",
    "    \n",
    "        num_examples = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "    \n",
    "        #\n",
    "    \n",
    "        # Initialize weights(i.e theta) with appropriate shape\n",
    "        self.weights = np.zeros((X.shape[1]))\n",
    "\n",
    "    \n",
    "        lambda_param = 0.1  # L2 regularization parameter\n",
    "    \n",
    "        # Perform gradient ascent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Define the linear hypothesis(z) first, to calculate z = theta * x\n",
    "            z  = np.dot(X,self.weights)\n",
    "      \n",
    "            # Output probability value by appplying sigmoid on z\n",
    "            y_pred = self.sigmoid(z)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Calculate the gradient values(i.e the gradient of the log likelihood function)\n",
    "            #This is a partial derivative of the function with respect to weights(theta)\n",
    "            # Adding L2 regularization term to the gradient\n",
    "            gradient = np.mean((y - y_pred) * X.T, axis=1) - 2 * lambda_param * self.weights\n",
    "        \n",
    "            # Update the weights using gradient ascent\n",
    "            self.weights +=  self.learning_rate * gradient\n",
    "        \n",
    "            # Calculating log likelihood\n",
    "            likelihood = self.log_likelihood(y, y_pred)\n",
    "\n",
    "            self.likelihoods.append(likelihood)\n",
    "\n",
    "        #\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        '''Predict probabilities for given X.\n",
    "        sigmoid returns value between 0 and 1.\n",
    "        Args:\n",
    "            X : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            probabilities: Numpy array (num_samples,)\n",
    "        '''\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Fit the model before prediction\")\n",
    "      \n",
    "        #\n",
    "               \n",
    "        z = np.dot(X,self.weights)\n",
    "        probabilities = self.sigmoid(z)\n",
    "        # probabilities.reshape(probabilities.shape[0],1)\n",
    "        \n",
    "        #\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        '''Predict/Classify X in classes\n",
    "        Args:\n",
    "            X         : Numpy array (num_samples, num_features)\n",
    "            threshold : scalar value above which prediction is 1 else 0\n",
    "        Returns:\n",
    "            binary_predictions : Numpy array (num_samples,)\n",
    "        '''\n",
    "        # Thresholding probability to predict binary values\n",
    "        \n",
    "        binary_predictions = np.array(list(map(lambda x: 1 if x>threshold else 0, self.predict_proba(X))))\n",
    "        \n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72f337-2f93-4e7c-82d9-5048bb16ebc0",
   "metadata": {},
   "source": [
    "Loading the Iris dataset to test our logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6a64463-08ee-44e7-8e91-ab8ddb5aef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target \n",
    "X = X[y != 2]\n",
    "y = y[y != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cedce409-abb1-4dc4-8180-e213a8b65453",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cff86b7-96ba-4125-91cc-d37838179ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "\n",
    "# Apply the scaler to the test data\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbd6313d-b18f-486e-8eb4-08411346b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of LogisticRegressionWithL2\n",
    "log_reg = LogisticRegressionWithL2(learning_rate=0.01, max_iterations=1000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "log_reg.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = log_reg.predict(X_test_std)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8627fde3-c774-4a8a-8025-0f465c49e6c1",
   "metadata": {},
   "source": [
    "# Question 2 (20 pts)\n",
    "\n",
    "Use the iris dataset (just the binary class Iris Setosa vs others), the K-fold cross-validation, metrics = accuracy and the logistic regression with L2 regularization.\n",
    "<br>\n",
    "You can use scikit.\n",
    "<br>\n",
    "Please estimate the best parameter C (the inverse of lambda) used for the regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "055578a0-87d7-47d3-97a1-d154d1688901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "996320ee-8cfb-462a-8040-b310b7ae2b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e63bdec0-4d1f-4064-9cf4-49954ba52fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=iris.iloc[:,0:4]\n",
    "y=iris.iloc[:,4]\n",
    "# Encode labels\n",
    "#y = np.array([1 if label == 'setosa' else 0 for label in y])\n",
    "def val(s):\n",
    "    if s=='setosa':\n",
    "        return 1\n",
    "    return 0\n",
    "y=np.array([val(x) for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab717e86-9ad7-462c-8dfb-d67f30a6e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31ac721b-d57d-415a-8a9b-4fc5a507737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler\n",
    "scaler = StandardScaler()\n",
    "X_train2=scaler.fit_transform(X_train2)\n",
    "X_test2=scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06188a64-b5ed-4bc8-9ea4-d26d96bb0d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.001\n",
      "Best Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define list of regularization parameters\n",
    "C = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 10000000]\n",
    "\n",
    "# Initialize best classifier and best C value\n",
    "best_classifier = LogisticRegression(random_state=0, class_weight='balanced', penalty='l2')\n",
    "best_C = 0\n",
    "best_accuracy = 0\n",
    "cv = 10\n",
    "\n",
    "# Perform cross-validation and parameter tuning\n",
    "for c in C:\n",
    "    avg_accuracy = 0\n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=10, shuffle=True)\n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        X_train_cv = X_train2[train_index]\n",
    "        X_val = X_train2[val_index]\n",
    "        y_train_cv = y_train2[train_index]\n",
    "        y_val = y_train2[val_index]\n",
    "        clf = LogisticRegression(random_state=0, class_weight='balanced', C=c, penalty='l2')\n",
    "        clf.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_val = clf.predict(X_val)\n",
    "        avg_accuracy += accuracy_score(y_val, y_pred_val)\n",
    "    avg_accuracy /= cv\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_C = c\n",
    "        best_classifier = clf\n",
    "\n",
    "# Print best C value and accuracy\n",
    "print('Best C:', best_C)\n",
    "print('Best Accuracy:', best_accuracy)\n",
    "\n",
    "# Testing\n",
    "y_pred = best_classifier.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
